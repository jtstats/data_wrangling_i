---
title: "Tidy Data"
author: "Jingyi"
date: "9/24/2019"
output: github_document
---

First, update tidyverse.
This only came out a week ago.

```{r}

#install.packages("tidyr")

#########
# I am not building a R package, but do I still need to do so?
#########

```


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load the library
library(tidyverse)

```

## Wide to long

```{r load the name and clean the names}

pulse_data = haven::read_sas("./data/public_pulse_data.sas7bdat") %>%
  janitor::clean_names()
pulse_data

```

With our new understanding of tidy data, we quickly recognize a problem: the BDI score is spread across four columns, which correspond to four observation times. We can fix this problem using pivot_longer:

```{r organize BDI score according to their observation times}

pulse_tidy_data = 
  pivot_longer(
    pulse_data,
    bdi_score_bl:bdi_score_12m,
    names_to = "visit",
    values_to = "bdi"
  )

pulse_tidy_data

```

This looks much better! However, now visit is an issue. The original column names were informative but we probably don’t need to keep the bdi_score_ prefix in each case. I’ll use an additional option in pivot_longer to address this:

```{r rename the observations}

pulse_tidy_data = 
  pivot_longer(
    pulse_data,
    bdi_score_bl:bdi_score_12m,
    names_to = "visit",
    names_prefix = "bdi_score_",
    values_to = "bdi")

pulse_tidy_data
 
```

In the preceding I’ve saved intermediate datasets to make each step clear.

While this can be a helpful crutch as you’re trying out code, it is generally bad practice. 

There are also some additional transformations needed to wrap up the data wrangling process, like changing bl to 00m for consistency across visits and converting visit to a factor variable.

 (It’s possible that you would want visit to be a numeric variable instead, which could be done with a different call to mutate.)
 
Lastly, it’s nice to organize the data into a reasonable order.

Altogether, then, the code below will import, tidy, and transform the PULSE dataset into a usable format:

```{r some modifications}

pulse_data = 
  haven::read_sas("./data/public_pulse_data.sas7bdat") %>%   
  # read data
  janitor::clean_names() %>%   
  # clean names 
  pivot_longer(  
    # put multiple columns into a new column and put the values of them to another column
    bdi_score_bl:bdi_score_12m,  
    # put these columns
    names_to = "visit",  
    # the column name is visit for time of observations (original column names)
    names_prefix = "bdi_score_",  
    # get rid of the prefix of the "observations" of the original column names
    values_to = "bdi") %>%  
  # put the values of each orginal columns to a new column called bdi
  select(id, visit, everything()) %>%  
  # select all variables in the dataset
  mutate(
    visit = replace(visit, visit == "bl", "00m"), 
    # changing bl to 00m for consistency across visits
    visit = factor(visit, levels = str_c(c("00", "01", "06", "12"), "m"))) %>%   
  # converting visit to a factor variable
  arrange(id, visit)
  # organize the data into order as id, visit, age, sex and bdi (ascending)
  
print(pulse_data, n = 12)
# print the dataset to see the first 12 observations

```

Before moving on, let’s revisit the group variable in the litters dataset:

```{r litters data revisit}

read_csv("./data/FAS_litters.csv", col_types = "ccddiiii") %>% 
  janitor::clean_names() %>% 
  count(group)

```

These data are also untidy: group encodes both dose and day of treatment! Time to fix that …

```{r tidy litters data}

litters_data = 
  # load the data
  read_csv("./data/FAS_litters.csv", col_types = "ccddiiii") %>% 
  # clean the names
  janitor::clean_names() %>% 
  # separate the group into dose and day of tx
  separate(group, into = c("dose", "day_of_tx"), sep = 3) %>% 
  # QUESTION: figure out this sep = 3
  
  mutate(
    # bring the strings in dose into their lower cases
    dose = str_to_lower(dose),
    # add a new column called wt_gain by this subtraction
    wt_gain = gd18_weight - gd0_weight) %>% 
  
  # arrange by litter number
  arrange(litter_number)
# Question: Why I can't proceed without this arrange()?
# Question solved. I forgot to take out the %>% 
  
#display the dataset
litters_data

```

##Leanring Assessment #1:

In the litters data, the variables gd0_weight and gd18_weight give the weight of the mother mouse on gestational days 0 and 18. 

Write a data cleaning chain that retains only litter_number and these columns; produces new variables gd and weight; and makes gd a numeric variable taking values 0 and 18 (for the last part, you might want to use recode …). 

Is this version “tidy”?

```{r}

# in litters_data dataset
litters_data %>% 
  
  # retains only litter_number, gd0_weight and gd18_weight
  select(litter_number, gd0_weight, gd18_weight) %>%
  
  # put gd0 and gd18 into one column called gd, and their values to a column called weight
  pivot_longer(
    gd0_weight:gd18_weight,
    names_to = "gd",
    values_to = "weight"
  ) %>% 
  
  # change the name of gd0_weight to 0 and so for 18, and put these 0 and 18's into gd column as observations
  mutate(gd = recode(gd, "gd0_weight" = 0, "gd18_weight" = 18))
  
```

In one sense, this is “tidy” because I have a variable for day and a variable for weight rather that using values in my variable names. However, it’s less useful if I’m interested in computing or analyzing weight gain during pregnancy.


##pivot_wider

We’ve been exclusively interested in tidying data, but we’ve admitted that sometimes untidy is better for human consumption. For that reason we’re going to take a short digression into untidying your tidy data.

The code below creates a tidy dataset that could result from an analysis. This is the correct format for additional analysis or visualization, but doesn’t facilitate quick comparisons for human readers.

```{r not for human dataset}

analysis_result = tibble(
  group = c("treatment", "treatment", "placebo", "placebo"),
  time = c("pre", "post", "pre", "post"),
  mean = c(4, 8, 3.5, 4)
)

analysis_result

```

An alternative presentation of the same data might have groups in rows, times in columns, and mean values in table cells. This is decidedly non-tidy; to get there from here we’ll need to use pivot_wider, which is the inverse of pivot_longer:

```{r make the not for human dataset for human}

pivot_wider(
  # dataset name
  analysis_result, 
  # put time into columns
  names_from = "time", 
  # put means into table cells
  values_from = "mean"
  # and leave groups in row
  )

```

We’re pretty much there now – in some cases you migth use select to reorder columns, and (depending on your goal) use knitr::kable() to produce a nicer table for reading.


## Binding rows

We’ve looked at single-table non-tidy data, but non-tidiness often stems from relevant data spread across multiple tables.

In the simplest case, these tables are basically the same and can be stacked to produce a tidy dataset.

That’s the setting in LotR_words.xlsx, where the word counts for different races and sexes in each movie in the trilogy are spread across distinct data rectangles.

To produce the desired tidy dataset, we first need to read each table and do some cleaning.

```{r}

fellowship_ring = 
  # read data in excel and for certain range
  readxl::read_excel("./data/LotR_Words.xlsx", range = "B3:D6") %>% 
  # add a column named movie and put the value of that column for the data we read here as fellowship_ring
  mutate(movie = "fellowship_ring") 

two_towers = 
  readxl::read_excel("./data/LotR_Words.xlsx", range = "F3:H6") %>%
  mutate(movie = "two_towers")

return_king = 
  readxl::read_excel("./data/LotR_Words.xlsx", range = "J3:L6") %>%
  mutate(movie = "return_king")

```

Here it was necessary to add a variable to each dataframe indicating the movie; that information had stored elsewhere in the original spreadsheet. 

As an aside, the three code snippets above are all basically the same except for the range and the movie name – later we’ll see a better way to handle cases like this by writing our own functions, but this works for now.

Once each table is ready to go, we can stack them up using bind_rows and tidy the result:

```{r}

lotr_tidy = 
  # bind the rows of these 3 dataframes
  bind_rows(fellowship_ring, two_towers, return_king) %>%
  # clean the names
  janitor::clean_names() %>%
  # make the female and male columns as data value in a new column named sex, and put the values of original female and male columns into a new column called words
  pivot_longer(
    female:male,
    names_to = "sex", 
    values_to = "words") %>%
  #change the data in race into their lower cases
  mutate(race = str_to_lower(race)) %>% 
  # show everything in lotr_tidy
  select(movie, everything()) 
  # select(everything()) just grabs everything, but select(movie, everything()) put the movie first and then everything
  
lotr_tidy

```

Having the data in this form will make it easier to make comparisons across movies, aggregate within races across the trilogy, and perform other analyses.


##Joining datasets

Data can be spread across multiple related tables, in which case it is necessary to combine or **join** them prior to analysis. We’ll focus on the problem of combining two tables only; combining three or more is done step-by-step using the same ideas.

There are four major ways join dataframes x and y:

* Inner: keeps data that appear in both x and y
* Left: keeps data that appear in x
* Right: keeps data that appear in y
* Full: keeps data that appear in either x or y

Left joins are the most common, because they add data from a smaller table y into a larger table x without removing anything from x.

As an example, consider the data tables in FAS_pups.csv and FAS_litters.csv, which are related through the Litter Number variable. 

The former contains data unique to each pup, and the latter contains data unique to each litter. 

We can combine these using a left join of litter data into pup data; doing so retains data on each pup and adds data in new columns.

```{r joining data pups}

pup_data = 
  # read pups dataset
  read_csv("./data/FAS_pups.csv", col_types = "ciiiii") %>%
  # clean names
  janitor::clean_names() %>%
  # add column sex and rename male as 1 and female as 2
  mutate(sex = recode(sex, `1` = "male", `2` = "female")) 

litter_data = 
  # read litters dataset
  read_csv("./data/FAS_litters.csv", col_types = "ccddiiii") %>%
  # clean names
  janitor::clean_names() %>%
  # remove pups_survive column
  select(-pups_survive) %>%
  # add wt_gain and subtraction as its values
  # let everything in groups in lower cases
  mutate(
    wt_gain = gd18_weight - gd0_weight,
    group = str_to_lower(group))

# left join pups and litter, organize by "litter_number"
# this will keep everything in pups and discard NAs in pups
fas_data = 
  left_join(pup_data, litter_data, by = "litter_number")
fas_data

```

We made the key explicit in the join. By default, the *_join functions in dplyr will try to determine the key(s) based on variable names in the datasets you want to join.

This is often but not always sufficient, and an extra step to make the key clear will help you and others reading your code.

Note that joining is not particularly amenable to the %>% operator because it is fundamentally non-linear: two separate datasets are coming together, rather than a single dataset being processed in a step-by-step fashion.

As a final point, the *_join functions are very much related to SQL syntax, but emphasize operations common to data analysis.

##Learning Assessment #2:

The datasets in this zip file contain de-identified responses to surveys included in past years of this course. Both contain a unique student identifier; the first has reponses to a question about operating systems, and the second has responses to questions about degree program and git experience. Write a code chunk that imports and cleans both datasets, and then joins them.

```{r Learning Assessment #2}

# read data
surv_os = read_csv("./data/surv_os.csv") %>% 
  # clean names
  janitor::clean_names() %>% 
  # rename each variables
  rename(id = what_is_your_uni, os = what_operating_system_do_you_use)

# read data
surv_pr_git = read_csv("./data/surv_program_git.csv") %>% 
  # clean names
  janitor::clean_names() %>% 
  # rename
  rename(
    id = what_is_your_uni,
    prog = what_is_your_degree_program,
    git_exp = which_most_accurately_describes_your_experience_with_git
  )

left_join(surv_os, surv_pr_git)
inner_join(surv_os, surv_pr_git)
anti_join(surv_os, surv_pr_git)
anti_join(surv_pr_git, surv_os)

```


##A quick note on names:

Up until very recently, folks were using gather and spread instead of pivot_longer and pivot_wider. The new functions were updated for good reasons; gather and spread will still exist, but they’re going to be less common over time.